---
title: "Customer Analytics - Customer Lifetime Valuation"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = FALSE,
	message = FALSE,
	warning = FALSE
)
```

Customer lifetime Value is the forecasting of net profits that connect to a specific customer during their lifetime relationship with a business. To state it more simply, CLV is the financial value of a customer's relationship over the lifetime of the organization's relationship with that customer.

Customer lifetime Value is important because it gives an idea of the amount of repeat business that can be expected from a specific customer. This knowledge assists the organizations in deciding how much they can profitably invest in buying a particular customer for their business.

Once how much a customer buys and the frequency of their purchases is determined, the organizations have a better understanding of how to manage their limited resources.

## Problem Statement
To understand the problem of Customer Lifetime Value and provide a data-driven solution through investigating the data and models to explain the relationship between Customer Lifetime Value (Target/Response) and the other variables (Predictors/Explanatory) along with interpreting the visuals, descriptive statistics, tests, and models in terms of the association between the explanatory variables and the response variable.

## Policy Profile Variables
- Customer: Unique ID assigned to customers
- Coverage: Policy coverage chosen by the customers
- Effective To Date: Maturity date of insurance policy plan
- Monthly Premium Auto: Monthly premium paid for the policy
- Months Since Last Claim: Number of months that passed since the last claim made by the customer
- Months Since Policy Inception: Number of months since the activation of policy plan
- Number of Policies: Total number of policies purchased 
- Policy Type: Type of policy under the main categories
- Policy: Category of policy plan adopted by the customer - personal, corporate or special
- Renew Offer Type: Class of renewal offer accepted by the customer
- Total Claim Amount: Total amount that can be claimed by the customer on/before policy maturity
- Customer Lifetime Value: Net profit generated by customers for the firm

## Customer Profile Variables
- State: State to which customers belong
- Response: Positive or negative response with regards to purchase of policy plans
- Education: Education received by the customers
- Employment Status: Customers' current employment status
- Gender: Gender of customers
- Income: Income level of customers
- Location Code: Type of residential area of customers
- Marital Status: Relationship status
- Number of Open Complaints: Number of unsolved complaints made by the customer
- Sales Channel: Channel via sales with a particular customer occurred
- Vehicle Class: Class to which the insured vehicle belongs 
- Vehicle Size: Size of the customers insured vehicle



### 1. Import Libraries
```{r echo=TRUE, message=FALSE, warning=FALSE}

library(tidyverse)
library(plotly)
library(Hmisc)
library(ggplot2)
library(naniar)
library(ggpubr)
library(PerformanceAnalytics)
library(corrplot)
library(plyr)
library(readr)
library(caret)
library(ISLR)
library(mlbench)
library(measures)
library(earth)
library(scales)
library(lmtest)
library(matrixStats)
library(repr)
library(lubridate)
library(olsrr)
library(superml)
library(MASS)
library(phenex)
library(modelr)
library(broom)
library(magrittr)
library(rpart)
```

### 2. Loading the dataset
```{r echo=TRUE, message=FALSE, warning=FALSE}
#Loading the dataset
data <- read.csv("Marketing-Customer-Value-Analysis.csv")
```


### 3. Understanding the dataset
```{r}
summary(data)
```
### 3.1 Frequency distribution of the data
```{r}
print(describe(data))
```
### 3.2 Check for null values
It is concluded that there are no missing values in the given dataset.
```{r}
gg_miss_var(data)
```


### 4. Transforming the dataset
```{r}
data$Effective.To.Date <- as.Date(data$Effective.To.Date, format = "%m/%d/%y")
head(data$Effective.To.Date)
```
#### Inside the dataset, the Effective to date column was not in date format. So we transformed the column.

#### 4.1 Creating new columns
```{r}

data = data %>% mutate(Total.Premium.Since.Last.Claim = Months.Since.Last.Claim*Monthly.Premium.Auto*Number.of.Policies)

data = data %>% mutate(Total.Monthly.Premium.Auto = Monthly.Premium.Auto*Number.of.Policies)
```
Total.Premium.Since.Last.Claim
- New feature is created by multiplying Month since last claim, monthly premium auto and number of policies purchased by the customers. this gives the amount that the company is supposed to get from that customer just after the last claim.

Total.Monthly.Premium
- This is the product of monthly premium to number of policies purchased by the customer. This is the total amount that a particular customer is expected to pay to the company.

### 5. Exploratory Data Analysis

#### 5.1 Correlation plot for numerical variables
```{r}
my_data <- data[, c(9,12,13,14,15,16,21,24)]
M = cor(my_data)
corrplot(M, method = 'number', type = 'lower', diag = TRUE)
```

#### 5.2 Frequency plots for contious variables.
```{r}
ii <- ggplot(data, mapping = aes(x=Income)) + 
  geom_histogram(fill="bisque",color="white",alpha=0.7) +
  geom_freqpoly() + 
  theme_minimal()

tc <- ggplot(data, mapping = aes(x=Total.Claim.Amount)) + 
  geom_histogram(fill="bisque",color="white",alpha=0.7) +
  geom_freqpoly()+
  theme_minimal()

mp <- ggplot(data, mapping = aes(x=Monthly.Premium.Auto)) + 
  geom_histogram(fill="bisque",color="white",alpha=0.7) +
  geom_freqpoly()+
  theme_minimal()

ms <- ggplot(data, mapping = aes(x=Months.Since.Last.Claim)) + 
  geom_histogram(fill="bisque",color="white",alpha=0.7) +
  geom_freqpoly()+
  theme_minimal()

msp <- ggplot(data, mapping = aes(x=Months.Since.Policy.Inception)) + 
  geom_histogram(fill="bisque",color="white",alpha=0.7) +
  geom_freqpoly()+
  theme_minimal()

tps <- ggplot(data, mapping = aes(x=Total.Monthly.Premium.Auto)) + 
  geom_histogram(fill="bisque",color="white",alpha=0.7) +
  geom_freqpoly()+
  theme_minimal()

figure0 <- ggarrange(ii,tc,mp,ms,msp,tps,
                     ncol = 2, nrow = 3)
figure0
```


#### Out of all the policies which policy is a customer more likely to buy?
Most of the customers prefer buying Personal L3 policy which comes under Personal Auto policy type.
```{r}
pp <- ggplot(data, aes(x=as.factor(Policy), fill=as.factor(Policy) )) + 
  geom_bar() +
  scale_fill_brewer(palette = "Set3") +
  labs(x='Policies taken by the customer') +
  theme_minimal()+
  theme(legend.position="none")

cpp <- ggplot(data = data, mapping = aes(x =Customer.Lifetime.Value , color =Policy , fill =Policy)) + 
       geom_histogram(position = 'dodge') + 
       scale_fill_brewer(palette = 'Set3') + 
       scale_color_brewer(palette = 'Set3') + 
       labs(x = 'Customer Lifetime Value', y = 'count')+
       theme_minimal()

figure1 <- ggarrange(pp,cpp,
                    ncol = 1, nrow = 2)
figure1
```


#### How does Monthly Premium auto and Months since last claim affect the Customer Life time value?
Monthly premium auto contributes significantly to the net profit generated by customers for the bank.
As the monthly premium of a policy rises the Customer Lifetime Value increases.
Customers who haven't claimed an amount for a long time will be preferred over the one's who have recently claimed an amount. 
```{r}

a <- ggplot(data, mapping = aes(x=Months.Since.Last.Claim, y=Customer.Lifetime.Value)) + 
  geom_point(color="lightblue") +
  geom_rug()+
  theme_minimal()


b <- ggplot(data, mapping = aes(x=Monthly.Premium.Auto, y=Customer.Lifetime.Value)) + 
  geom_point(color="lightblue") +
  geom_rug()+
  theme_minimal()


figure2 <- ggarrange(a, b,
                    ncol = 2, nrow = 1)
figure2
```


#### Which sales channel is more likely to increase the customer life time value?
The agents have played a major role in promoting and selling insurance policies to customers. The agents strive hard to deliver the best insurance policies and services to their customers and hence are preferred over all other sales channel. 
```{r}

ct <- ggplot(data, aes(x=as.factor(Sales.Channel), fill=as.factor(Sales.Channel) )) + 
  geom_bar() +
  scale_fill_brewer(palette = "Set3") +
  labs(x='Policies taken by the customer') +
  theme_minimal()+
  theme(legend.position="none")


hist <- ggplot(data = data, mapping = aes(x =Customer.Lifetime.Value , color =Sales.Channel , fill = Sales.Channel)) + 
       geom_histogram(position = 'dodge') + 
       scale_fill_brewer(palette = 'Set3') + 
       scale_color_brewer(palette = 'Set3') + 
       labs(x = 'Customer Lifetime Value', y = 'Sales Channel Count')+
       theme_minimal()


figure3 <- ggarrange(ct, hist,
                    ncol = 1, nrow = 2)
figure3

```

#### Do the number of policies affect Total claim amount and CLV?
Number of policies does not have a significant impact on the total claim amount but it seems to affect the CLV values. It is seen that customers prefer buying two policies. However, they don't go beyond buying two because in the end they can claim an amount from just one policy so there's absolutely no need to buy more policies and pay monthly premium's for them.
```{r}
tn <- ggplot(data,mapping=aes(y=Total.Claim.Amount , x= Number.of.Policies))+
  geom_point(color="lightblue")+
  theme_minimal()

cn <- ggplot(data,mapping=aes(y=Customer.Lifetime.Value , x= Number.of.Policies))+
  geom_point(color="lightblue")+
  theme_minimal()


figure4 <- ggarrange(tn, cn,
                    ncol = 1, nrow = 2)
figure4
```

#### Are men more likely to buy vehicle insurance policies? Is there a gender bias?
There is no gender bias. Both men and women are equally likely to buy Vehicle insurance policies.
The 'ANOVA' test is performed to understand if there exists a significant difference between the means of the two groups (Male and Female) with respect to the CLV.
Since, p-value > 0.05 we fail to reject the null hypothesis and hence there exists no significant difference between the means of the two groups (Male and Female).
```{r}
fg <- ggplot(data, aes(x=factor(1), fill=Gender))+
  geom_bar(width = 1)+
  coord_polar("y")+
  scale_fill_brewer(palette = "Set3") +
  labs(x = ' ', y = ' ')+
  theme_minimal()

gc <- ggplot(data, mapping = aes(x=Gender, y=Customer.Lifetime.Value)) + 
  geom_boxplot()+
  theme_minimal()

figure5 <- ggarrange(fg, gc,
                    ncol = 2, nrow = 1)
figure5

#ANOVA Test
res.aov <- aov(Customer.Lifetime.Value ~ Gender, data = data)
res.aov
#Summary of ANOVA Test
summary(res.aov)
```


#### What is the response of the customers with regards to the policy plan?
85.7% of the responses recorded are negative.
```{r}
rr <- ggplot(data, aes(x=as.factor(Response), fill=as.factor(Response) )) + 
  geom_bar() +
  scale_fill_brewer(palette = "Set3") +
  labs(x='Response given by the customer') +
  theme_minimal()+
  theme(legend.position="none")

rc <- ggplot(data = data, mapping = aes(x =Customer.Lifetime.Value , color =Response , fill =Response)) + 
       geom_histogram(position = 'dodge') + 
       scale_fill_brewer(palette = 'Set3') + 
       scale_color_brewer(palette = 'Set3') + 
       labs(x = 'Customer Lifetime Value', y = 'Response Count')+
       theme_minimal()

figure6 <- ggarrange(rr, rc,
                    ncol = 1, nrow = 2)
figure6
```

#### Does the vehicle size or class have a role to play in increasing the CLV?
Through ANOVA test, we infer that there is no significant difference in the means of the groups of vehicle size with respect to CLV. It is also observed that the luxury class vehicles have higher CLV compared to other vehicle classes.
```{r}
vc <- ggplot(data, mapping = aes(x=Vehicle.Size, y=Customer.Lifetime.Value)) + 
  geom_boxplot()+
  theme_minimal()

vcc <- ggplot(data, mapping = aes(x=Vehicle.Class, y=Customer.Lifetime.Value)) + 
  geom_boxplot()+
  theme_minimal()

figure7 <- ggarrange(vc, vcc,
                    ncol = 1, nrow = 2)
figure7

#ANOVA Test
res.aov <- aov(Customer.Lifetime.Value ~ Vehicle.Size, data = data)
res.aov
#Summary of ANOVA Test
summary(res.aov)
```


#### Is vehicle size and class related to Monthly premium auto?
The class of the car is considered to be one of the important factors while calculating the monthly premium auto paid towards car insurance company. Luxury cars are more vulnerable to theft, that's why customers who own luxury cars are likely to get their cars insured and hence contribute to higher monthly premium auto.
Vehicle size does not contribute to monthly premium auto. This is  (ANOVA Test).
```{r}
vm <- ggplot(data, mapping = aes(x=Vehicle.Size, y=Monthly.Premium.Auto)) + 
  geom_boxplot()+
  theme_minimal()

vvm <- ggplot(data, mapping = aes(x=Vehicle.Class, y=Monthly.Premium.Auto)) + 
  geom_boxplot()+
  theme_minimal()

figure8 <- ggarrange(vm, vvm,
                    ncol = 1, nrow = 2)
figure8

#ANOVA Test
res.aov <- aov(Monthly.Premium.Auto ~ Vehicle.Size, data = data)
res.aov
#Summary of ANOVA Test
summary(res.aov)
```


#### Does the bank resolve the complaints raised by its customers and how does it affect the CLV?
Yes, the bank tries to resolve the complaints raised by its customers. Most of the customers have no pending complaints. We can see that the CLV value decreases as the number of pending complaints increases.
```{r}
nn <- ggplot(data, mapping = aes(x=Number.of.Open.Complaints, fill=as.factor(Number.of.Open.Complaints))) + 
  geom_bar() +
  scale_fill_brewer(palette = "Set3") +
  theme_minimal()+
  theme(legend.position="none")+
  labs(x='Number of pending Complaints') 
  

cnn <- ggplot(data,mapping=aes(y=Customer.Lifetime.Value , x= Number.of.Open.Complaints))+
  geom_point(color="lightblue")+
  theme_minimal()


figure9 <- ggarrange(nn, cnn,
                    ncol = 1, nrow = 2)
figure9
```


#### What is the relation between Income of the customer and Customer lifetime value?
Customers who have no income are unemployed but still contribute to high customer lifetime value. This fact cannot be justified from the data provided. Hence additional data on secondary sources of income must be provided to validate the statement.
```{r}
ei <- ggplot(data = data, mapping = aes(x=EmploymentStatus,y=Income)) + 
  geom_boxplot()+
  theme_minimal()

ic <- ggplot(data, aes(x=Income, y=Customer.Lifetime.Value)) +
geom_point(color="lightblue")+
theme_minimal()

figure10 <- ggarrange(ei,ic,
                      ncol = 1, nrow = 2)
figure10
```

#### Most of the unemployed customers are single.
```{r}
ggplot(data, aes(x=EmploymentStatus,fill=Marital.Status))+
  geom_bar(stat="count", position='fill',width = 0.5)+
  scale_fill_brewer(palette = "Set3") +
  theme_minimal()
```

#### Retired customers have given more positive response as compared to their negative response with regard to purchase of policy plans.They prefer to purchase policies through agents. Majority of them are living in suburban area.
```{r}
a2 <- ggplot(data, aes(x=EmploymentStatus,fill=Location.Code))+
  geom_bar(stat="count", position='fill',width = 0.5)+
  scale_fill_brewer(palette = "Set3") +
  theme_minimal()

b2 <- ggplot(data, aes(x=EmploymentStatus,fill=Sales.Channel))+
  geom_bar(stat="count", position='fill',width = 0.5)+
  scale_fill_brewer(palette = "Set3") +
  theme_minimal()

c2 <- ggplot(data, aes(x=EmploymentStatus,fill=Response))+
  geom_bar(stat="count", position='fill',width = 0.5)+
  scale_fill_brewer(palette = "Set3") +
  theme_minimal()

figure12 <- ggarrange(a2,b2,c2,
                     ncol = 1, nrow = 3)
figure12
```


#### Renewal offer type 4 has complete negative feedback
```{r}
ggplot(data, aes(x=Renew.Offer.Type,fill=Response))+
  geom_bar(stat="count", position='fill',width = 0.5)+
  scale_fill_brewer(palette = "Set3") +
  theme_minimal()
```


#### Sales happening through agents have higher positive purchase response.
```{r}
ggplot(data, aes(x=Response,fill=Sales.Channel))+
  geom_bar(stat="count", position='fill',width = 0.5)+
  scale_fill_brewer(palette = "Set3") +
  theme_minimal()
```

#### Retired, Medical Leave, Disabled and Unemployed customers accept renewal offer 1 where as majority of the employed people accept offer 2
```{r}
ggplot(data, aes(x=EmploymentStatus,fill=Renew.Offer.Type))+
  geom_bar(stat="count", position='fill',width = 0.5)+
  scale_fill_brewer(palette = "Set3") +
  theme_minimal()
```


#### The employment status of customers in all the 5 state doesnot differ significantly, however California and Oregon contribute to highest CLV values.   
```{r}
d7 <-ggplot(data, aes(x=State,fill=EmploymentStatus))+
  geom_bar(stat="count", position='fill',width = 0.5)+
  scale_fill_brewer(palette = "Set3") +
  theme_minimal()

e7 <-ggplot(data, aes(x=State, y=Customer.Lifetime.Value, fill=State)) +
  geom_bar(stat="identity")+
  scale_fill_brewer(palette = "Set3") +
  theme_minimal()

figure17 <- ggarrange(d7,e7,
                     ncol = 1, nrow = 2)
figure17
```


#### Unemployed customers have no income whereas all employed customers have high incomes.
```{r}
b8 <- c(0,25000,50000,75000,100000)
names <- c("low","medium","high","very high")
data$Income.cut1 <- cut(data$Income, breaks= b8, labels = names)

ggplot(data, aes(x=Income.cut1,fill=EmploymentStatus))+
  geom_bar(stat="count", position='fill',width = 0.5)+
  scale_fill_brewer(palette = "Set3") +
  xlab("Income")+
  theme_minimal()
  
  
```


#### Renewal offer 1 contributes to higher CLV values.
```{r}
ggplot(data, aes(x=Renew.Offer.Type, y=Customer.Lifetime.Value, fill = Renew.Offer.Type)) +
  geom_bar(stat="identity")+
  scale_fill_brewer(palette = "Set3") +
  theme_minimal()

```


#### More customers reside in suburban region contributing to more CLV.
```{r}
ggplot(data, aes(x=Location.Code, y=Customer.Lifetime.Value,fill=Location.Code)) +
  geom_bar(stat="identity")+
  scale_fill_brewer(palette = "Set3") +
  theme_minimal()

```

#### Doctors are paying low monthly premium amounts compared to other.
```{r}
b9 <- c(0,50,100,200,300)
names <- c("low","medium","high","very high")
data$Monthly.Premium.Auto_bin <- cut(data$Monthly.Premium.Auto, breaks= b9, labels = names)

m9 <- ggplot(data, aes(Monthly.Premium.Auto_bin,fill=Education))+
  geom_bar(stat="count", position='fill',width = 0.5)+
  scale_fill_brewer(palette = "Set3") +
  theme_minimal()

ep9 <- ggplot(data, aes(Education,fill=Policy))+
  geom_bar(stat="count", position='fill',width = 0.5)+
  scale_fill_brewer(palette = "Set3") +
  theme_minimal()

figure21 <- ggarrange(m9,ep9,
                     ncol = 1, nrow = 2)
figure21
```

#### Log Distrubution plot for Customer lifetime value.

```{r}
data$CLV=log(data$Customer.Lifetime.Value)

c12 <- ggplot(data, mapping = aes(x=CLV)) + 
  geom_histogram(fill="bisque",color="white",alpha=0.7) +
  geom_freqpoly()+
  labs(title = "CLV with log transformation")+
  theme_minimal()

cc12 <- ggplot(data, mapping = aes(x=Customer.Lifetime.Value)) + 
  geom_histogram(fill="bisque",color="white",alpha=0.7) +
  geom_freqpoly()+
  labs(title = "CLV without log transformation")+
  theme_minimal()
  
figure22 <- ggarrange(cc12,c12,
                     ncol = 1, nrow = 2)
figure22
```
The Log Transformation is applied on CLV to transform its skewed distribution to approximately normal. This makes CLV more interpretable 

- *How to Identify the most profitable customers?*

The most profitable customers for the insurance company are those employed with high income. They contribute to high customer lifetime values.

- *How to segment profitable customers?*

Profitable customers can be segmented on the basis of months since last claim, monthly premium auto and number of policies purchased.

- *Should the company invest in its marketing strategies?*

The response from the customers about the purchase of policy is negative in most cases. It is suggested that if the company invest in their customer service, offers provided, it will help in gaining positive response and hence  acquire more customers.


```{r}
data <- data[ -c(25,26,27,28,29) ]
```


### 6. Modeling


### 6.1 Converting Categorical Variables to Factors

```{r}
data$"State" <- as.factor(data$"State")
data$"Response" <- as.factor(data$"Response")
data$"Coverage" <- as.factor(data$"Coverage")
data$"Education" <- as.factor(data$"Education")
data$"EmploymentStatus" <- as.factor(data$"EmploymentStatus")
data$"Gender" <- as.factor(data$"Gender")
data$"Location.Code"  <- as.factor(data$"Location.Code" )
data$"Marital.Status"  <- as.factor(data$"Marital.Status")
data$"Policy.Type"  <- as.factor(data$"Policy.Type")
data$"Renew.Offer.Type"  <- as.factor(data$"Renew.Offer.Type")
data$"Policy"  <- as.factor(data$"Policy")
data$"Sales.Channel"  <- as.factor(data$"Sales.Channel")
data$"Vehicle.Class"  <- as.factor(data$"Vehicle.Class")
data$"Vehicle.Size"  <- as.factor(data$"Vehicle.Size")

```
All the categorical variables are converted to factors.

### 6.2 Converting Numerical Variables to Factors

```{r}
data$"Number.of.Open.Complaints" <- as.factor(data$"Number.of.Open.Complaints")
data$"Number.of.Policies" <- as.factor(data$"Number.of.Policies")
```
Even though number of open complaints and number of policies are numerical variables, they only have 6 and 9 levels respectively. So they are also converted to factors.

```{r}
str(data)
```

### 6.3 Feature Engineering
#### Removing 'Customer' AND 'Effective.To.Date' Variables

```{r}
CLV_trans <- data[ -c(1,6) ]
head(CLV_trans)
```
The 'Customer' variable is removed since it is just ids. For 'Effective.To.Date', it only has 2 months, Jan and Feb in the year 2011. So this is also removed.

#### Binning and Capping

We tried binning three variables Total Claim Amount, Monthly Premium Auto and Income. But, Binning did not help us improve the model performance. Hence, we did not use it in our final model. 


We capped Total Claim Amount, Monthly Premium Auto and Income at 0.97 (97th Quartile). Capping too did not help us improve the model performance. Hence, we did not use it in our final model. 


#### Splitting of data into Train and Test data
```{r}

train_size = floor(0.75*nrow(CLV_trans)) 

set.seed(123)  
train_CLV = sample(seq_len(nrow(CLV_trans)),size = train_size)  
train =CLV_trans[train_CLV,] 
test=CLV_trans[-train_CLV,]  
dim(train)
dim(test)
```
The data is split into train and test with a ratio 3:1

#### Scaling

Scaling does not significantly impact the regression model. Hence, we have tried but not incorporated it in our modeling.


### 6.4 Model Building

#### 1. Multiple Linear Regression Model

```{r}
model_1 <- lm(log(Customer.Lifetime.Value) ~., data = train)
summary(model_1)
```
The above linear model contains all the features. 

When we plot the density plot for 'Customer.Lifetime.Value', its visible that the graph is skewed towards the left.To tackle this we take log(Customer.Lifetime.Value).

The log transformation reduces or removes the skewness of our original data. Log transformation also de-emphasizes outliers and allows us to potentially obtain a bell-shaped distribution. The idea is that taking the log of the data can restore symmetry to the data.

The adj R-squared value after using log is found to be 0.8977


```{r}
eval_fns<-function(train_x,predictions,actual){
  n=length(actual)
  absolute=abs(actual-predictions)
  MAE=sum(absolute)/n
  
  square_value=(actual-predictions)**2
  MSE=sum(square_value)/n
  RMSE=sqrt(MSE)
  
  print(paste("RMSE ",RMSE))
  
  mean_value=mean(actual)
  differ=(actual-mean_value)**2
  num=sum(square_value)
  dinom=sum(differ)
  R2=1-(num/dinom)
  
  
  k=ncol(train_x)-1
  num1=(1-R2)*(n-1)
  dinom1=n-k-1
  adj_R2=1-(num1/dinom1)
  
  print(paste("Adj R2 " ,adj_R2))
  
}

pred1 <- predict(model_1,newdata = test)
pred2 <- predict(model_1,newdata = train)

eval_fns(train,exp(pred2),train$Customer.Lifetime.Value)
eval_fns(test,exp(pred1),test$Customer.Lifetime.Value)

```

To predict the RMSE and adj R squared for train and test together we created a function eval_fns. Since log was taken for Customer.Lifetime.Value, exponential of the predict function is taken.
Adj R-squared for train is 0.670 which means that 67% of the variation in Customer.Lifetime.Value can be explained by the independent variables included in the model.
Adj R-squared for test is 0.649 which means that 64.9% of the variation in Customer.Lifetime.Value can be explained by the independent variables included in the model.


#### 2. Multiple Linear Regression Using Feature Selection

```{r}
model <- train(Customer.Lifetime.Value ~ .,method="rpart", data=train)
importance <- varImp(model, scale=FALSE)
plot(importance,top=15)
```

The features are selected based on the importance score and the significant p-values obtained from model_1.
We use the method rpart to perform feature selection.Rpart means splitting the dataset recursively, which means that the subsets that arise from a split are further split until a predetermined termination criterion is reached.

```{r}
model_2 <- lm(formula = log(Customer.Lifetime.Value) ~  + Total.Claim.Amount + Coverage + Education + EmploymentStatus + Marital.Status + Monthly.Premium.Auto  + Number.of.Open.Complaints  + Number.of.Policies  + Policy + Renew.Offer.Type + Vehicle.Class, data = train)
summary(model_2)
```
This model includes features from the importance score and p-values.
here Adj R2 is found to 0.8973.


```{r}
pred1 <- predict(model_2,newdata = test)
pred2 <- predict(model_2,newdata = train)

eval_fns(train,exp(pred2),train$Customer.Lifetime.Value)
eval_fns(test,exp(pred1),test$Customer.Lifetime.Value)
```
Adj R-squared for train is 0.669 which means that 66.9% of the variation in Customer.Lifetime.Value can be explained by the independent variables included in the model.
Adj R-squared for test is 0.649 which means that 64.9% of the variation in Customer.Lifetime.Value can be explained by the independent variables included in the model.


#### 3. Polynomial Regression Model

```{r}
model_4 <- lm(formula = log(Customer.Lifetime.Value) ~  + poly(Total.Claim.Amount,3,raw = TRUE) + Coverage + Education + EmploymentStatus + Marital.Status + poly(Monthly.Premium.Auto,3,raw = TRUE)  + Number.of.Open.Complaints +  Number.of.Policies  + Policy + Renew.Offer.Type + Vehicle.Class, data = train)
summary(model_4)

```

Polynomial regression fits a nonlinear relationship between the value of x and the corresponding conditional mean of y, denoted E(y|x). Basically it adds the quadratic or polynomial terms to the regression.

We have chosen polynomial regression as our 3rd model as it provides the best approximation of the relationship between the dependent and independent variables. Also the polynomial regression fits a wide range of curvature.

Adj R-squared value is found out to be 0.9023

```{r}
pred1 <- predict(model_4,newdata = test)
pred2 <- predict(model_4,newdata = train)

eval_fns(train,exp(pred2),train$Customer.Lifetime.Value)
eval_fns(test,exp(pred1),test$Customer.Lifetime.Value)

```
Adj R-squared for train is 0.683 which means that 68.3% of the variation in Customer.Lifetime.Value can be explained by the independent variables included in the model.
Adj R-squared for test is 0.669 which means that 66.9% of the variation in Customer.Lifetime.Value can be explained by the independent variables included in the model.


#### 4. Weighted Polynomial Regression Model

```{r}
model_1.0 <- lm(formula = log(Customer.Lifetime.Value) ~ Number.of.Policies +
Monthly.Premium.Auto + Total.Claim.Amount + Vehicle.Class + State +
Coverage + Education + Gender +
EmploymentStatus + Marital.Status + Months.Since.Last.Claim+
Number.of.Open.Complaints + Policy.Type +
Renew.Offer.Type+
Sales.Channel + Vehicle.Size
+ I(Monthly.Premium.Auto^2) +
I(Total.Claim.Amount^2) + I(Months.Since.Last.Claim^2)
, data = train)
summary(model_1.0)
```

```{r}
#Test for heteroscedasticity
#create residual vs. fitted plot
plot(fitted(model_1.0), resid(model_1.0), xlab='Fitted Values', ylab='Residuals')

#add a horizontal line at 0
abline(0,0)

#perform Breusch-Pagan test
bptest(model_1.0)
```
The residuals are not distributed with equal variance throughout the plot.

Heteroskedasticity refers to situations where the variance of the residuals is unequal over a range of measured values. When running a regression analysis, heteroskedasticity results in an unequal scatter of the residuals (also known as the error term). 

To formally test for heteroscedasticity, we  perform a Breusch-Pagan test. The Breusch-Pagan test uses the following null and alternative hypotheses:
- Null Hypothesis (H0): Homoscedasticity is present (the residuals are distributed with equal variance)
- Alternative Hypothesis (HA): Heteroscedasticity is present (the residuals are not distributed with equal variance)
Since the p-value from the test is less than 2.2e-16 we will reject the null hypothesis and conclude that heteroscedasticity is a problem in this model.


Since heteroscedasticity is present, we will perform weighted least squares by defining the weights in such a way that the observations with lower variance are given more weight.
```{r}
#define weights to use
wt <- 1 / lm(abs(model_1.0$residuals) ~ model_1.0$fitted.values)$fitted.values^2

#perform weighted least squares regression
wls_model <- lm(formula = log(Customer.Lifetime.Value) ~ Number.of.Policies +
Monthly.Premium.Auto + Total.Claim.Amount + Vehicle.Class + State +
Coverage + Education + Gender +
EmploymentStatus + Marital.Status + Months.Since.Last.Claim+
Number.of.Open.Complaints + Policy.Type +
Renew.Offer.Type+
Sales.Channel + Vehicle.Size
+ I(Monthly.Premium.Auto^2) +
I(Total.Claim.Amount^2) + I(Months.Since.Last.Claim^2)
, data=train, weights=wt)
summary(wls_model)

```


One of the key assumptions of regression is that the residuals are distributed with equal variance at each level of the predictor variable. This assumption is known as homoscedasticity.
When this assumption is violated, we say that heteroscedasticity is present in the residuals. When this occurs, the results of the regression become unreliable.

We have handled this issue by using weighted polynomial regression, which places weights on the observations such that those with small error variance are given more weight since they contain more information compared to observations with larger error variance.

From summary the adj R-squared value is 0.9632


```{r}
pred1 <- predict(wls_model,newdata = test)
pred2 <- predict(wls_model,newdata = train)

eval_fns(train,exp(pred2),train$Customer.Lifetime.Value)
eval_fns(test,exp(pred1),test$Customer.Lifetime.Value)

```

Adj R-squared for train is 0.679 which means that 67.9% of the variation in Customer.Lifetime.Value can be explained by the independent variables included in the model.
Adj R-squared for test is 0.664 which means that 66.4% of the variation in Customer.Lifetime.Value can be explained by the independent variables included in the model.


#### Residual Plots without weights
```{r}
par(mfrow=c(2,2))
plot(model_1.0)
```


*Fitted vs Residual graph*

Residuals plots should be random in nature and there should not be any pattern in the graph. The average of the residual plot should be close to zero. From the above plot, we can see that the red trend line is almost at zero.

*Normal Q-Q Plot*

Q-Q plot shows whether the residuals are normally distributed. Ideally, the plot should be on the dotted line. If the Q-Q plot is not on the line then models need to be reworked to make the residual normal. In the above plot, we see that points in the middle lie on the line whereas points in the start and towards the end does not lie on the line.

*Scale-Location*

This shows how the residuals are spread and whether the residuals have an equal variance or not.

*Residuals vs Leverage*

The plot helps to find influential observations. Here we need to check for points that are outside the dashed line. A point outside the dashed line will be influential point and removal of that will affect the regression coefficients.


So to improve the Normal Q-Q Plot we perform the weighted polynomial regression.
The results after are given below:

#### Weighted Residual Plots
```{r}
par(mfrow=c(2,2))
plot(wls_model)
```

Here we can see most of the points lie on the dotted line for the Normal Q-Q plot.

#### 5. Stepwise Regression
```{r}
step.model_1 <- stepAIC(model_1, direction = "both", 
                        trace = FALSE)
summary(step.model_1)

car::vif(step.model_1)
```


We have tried bidirectional stepwise linear regression which does not improve the model.



#### 6. Decision Tree
```{r}
fit_2 <- rpart((Customer.Lifetime.Value) ~ Number.of.Policies +
(Monthly.Premium.Auto) + (Total.Claim.Amount) + Vehicle.Class + State +
Coverage + Education + Gender +
EmploymentStatus + Marital.Status + Months.Since.Last.Claim+
Number.of.Open.Complaints + Policy.Type +
Renew.Offer.Type+
Sales.Channel + Vehicle.Size
,
method = "anova", data = train)

pred1 <- predict(fit_2,newdata = test)
pred2 <- predict(fit_2,newdata = train)

eval_fns(train,(pred2),train$Customer.Lifetime.Value)
eval_fns(test,(pred1),test$Customer.Lifetime.Value)

```

Decision trees provide an effective method of Decision Making because they clearly lay out the problem so that all options can be challenged. They allow us to analyze fully the possible consequences of a decision and provides a framework to quantify the values of outcomes and the probabilities of achieving them.

This model does not improve the adj R-squared value compared to the weighted polynomial regression model. 


### 8. Conclusion

We go with weighted polynomial regression model since it gives better value for adj R-squared and RMSE.This model explains most of the variation in target variable i.e.  Customer.Lifetime.Value with respect to the independent variables.

### 9. Solutions preferred.
- The employment status of customers in all the 5 states does not differ significantly, however California and Oregon contribute to highest CLV values. Hence the company should try to up their game in the other states with lesser CLV values.

- The response from the customers about the purchase of policy is negative in most cases. It is suggested that if the company invests in improving their customer service, offers provided, it will help in gaining positive response and hence  acquire more customers.

- Personal policy L3 is able to contribute to highest CLV values by attracting a large customer base. The company should try to advertise the other policies to get them recognized in the market.

- Doctors and Master degree holders although few in number compared to other categories contribute to almost equal amounts of CLV. Hence, the company must draw the attention of these categories towards their policies. And more focus should be given to education as more and more doctors and post graduate holders will contribute to higher CLV.

- Target customers based on their monthly premium auto and months since last claim.

- Sales channels like call center and company websites do not contribute significantly to the CLV. So special emphasis can be given to improve these channels so that more and more customers do not have to just relay on agents to buy a policy.


### 10. Additional information required.

- Customers who have no income are unemployed but still contribute to high customer lifetime value. This fact cannot be justified from the data provided. Hence additional data on secondary sources of income must be provided to validate the statement.

- The age of the customer can contribute to the CLV value. Hence this can be explored only if the data is available. Also we can find out if policies are sanctioned to genuine customers(legal age to drive is 16 in the given states of US)

- The date of policy purchase is necessary to understand the loyalty of the customer as the number of years they are associated with the company can be calculated and its relation with CLV can be understood.

### 11. Work Division

We divided the work equally amongst ourselves. Each of us had come up with our own EDA plots and tried many models. The models with better RMSE and Adj R-Squared value were selected

"*Teamwork is the secret that makes common people achieve uncommon results :P*"

### 12. Group-6

- Reshma Dua (20BDA05)
- Manu Tom (20BDA23)
- Cleon Lobo (20BDA28)
- Girisha Manocha (20BDA29)
- Keerthana Sajeevan (20BDA39)
- NagaVarun S.N (20BDA67)

### 13. Resources
- Kaggle.com
- Github.com
- towardsdatascience.com
- medium.com
- R Documentation
- statology.org
- geeksforgeeks.com
- datascienceplus
- analyticsvidhya
- R pubs
- ISLR
- datacamp
- KDnuggets



